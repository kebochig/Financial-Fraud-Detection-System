{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Fraud Detection\n",
    "\n",
    "This notebook focuses on extracting meaningful features from the parsed transaction logs to prepare data for anomaly detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from features.feature_engineering import FeatureExtractor\n",
    "from utils.config import Config\n",
    "from utils.visualization import VisualizationUtils\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config()\n",
    "viz = VisualizationUtils()\n",
    "\n",
    "# Load parsed transaction data\n",
    "data_path = '../data/parsed_transactions.csv'\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df)} parsed transactions\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"Parsed transaction data not found. Please run the data exploration notebook first.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Basic data info\n",
    "    print(\"Dataset Info:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nData Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(\"No missing values found\")\n",
    "    \n",
    "    # Sample data\n",
    "    print(\"\\nSample Data:\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature extractor\n",
    "feature_extractor = FeatureExtractor(config)\n",
    "\n",
    "print(\"Available feature categories:\")\n",
    "categories = feature_extractor.get_feature_categories()\n",
    "for category, features in categories.items():\n",
    "    print(f\"  {category}: {len(features)} features\")\n",
    "    if len(features) <= 10:\n",
    "        print(f\"    {', '.join(features)}\")\n",
    "    else:\n",
    "        print(f\"    {', '.join(features[:5])}... and {len(features)-5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"Extracting basic features...\")\n",
    "    \n",
    "    # Basic features\n",
    "    basic_features = feature_extractor.extract_basic_features(df)\n",
    "    print(f\"Extracted {len(basic_features.columns)} basic features\")\n",
    "    \n",
    "    # Display sample of basic features\n",
    "    print(\"\\nSample Basic Features:\")\n",
    "    display(basic_features.head())\n",
    "    \n",
    "    # Basic feature statistics\n",
    "    print(\"\\nBasic Feature Statistics:\")\n",
    "    display(basic_features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract User Behavioral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"Extracting user behavioral features...\")\n",
    "    \n",
    "    # User behavioral features\n",
    "    user_features = feature_extractor.extract_user_behavioral_features(df)\n",
    "    print(f\"Extracted {len(user_features.columns)} user behavioral features\")\n",
    "    \n",
    "    # Display sample of user features\n",
    "    print(\"\\nSample User Behavioral Features:\")\n",
    "    display(user_features.head())\n",
    "    \n",
    "    # User feature statistics\n",
    "    print(\"\\nUser Behavioral Feature Statistics:\")\n",
    "    display(user_features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"Extracting temporal features...\")\n",
    "    \n",
    "    # Temporal features\n",
    "    temporal_features = feature_extractor.extract_temporal_features(df)\n",
    "    print(f\"Extracted {len(temporal_features.columns)} temporal features\")\n",
    "    \n",
    "    # Display sample of temporal features\n",
    "    print(\"\\nSample Temporal Features:\")\n",
    "    display(temporal_features.head())\n",
    "    \n",
    "    # Temporal feature statistics\n",
    "    print(\"\\nTemporal Feature Statistics:\")\n",
    "    display(temporal_features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Contextual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"Extracting contextual features...\")\n",
    "    \n",
    "    # Contextual features\n",
    "    contextual_features = feature_extractor.extract_contextual_features(df)\n",
    "    print(f\"Extracted {len(contextual_features.columns)} contextual features\")\n",
    "    \n",
    "    # Display sample of contextual features\n",
    "    print(\"\\nSample Contextual Features:\")\n",
    "    display(contextual_features.head())\n",
    "    \n",
    "    # Contextual feature statistics\n",
    "    print(\"\\nContextual Feature Statistics:\")\n",
    "    display(contextual_features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"Combining all features...\")\n",
    "    \n",
    "    # Extract all features at once\n",
    "    features_df, metadata = feature_extractor.extract_all_features(df)\n",
    "    \n",
    "    print(f\"Total features extracted: {len(features_df.columns)}\")\n",
    "    print(f\"Feature metadata categories: {list(metadata.keys())}\")\n",
    "    \n",
    "    # Display feature metadata summary\n",
    "    print(\"\\nFeature Summary by Category:\")\n",
    "    for category, feature_list in metadata.items():\n",
    "        print(f\"  {category}: {len(feature_list)} features\")\n",
    "    \n",
    "    # Display combined features sample\n",
    "    print(\"\\nSample Combined Features:\")\n",
    "    display(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'features_df' in locals():\n",
    "    print(\"Analyzing feature quality...\")\n",
    "    \n",
    "    # Check for missing values in features\n",
    "    feature_missing = features_df.isnull().sum()\n",
    "    if feature_missing.sum() > 0:\n",
    "        print(\"\\nFeatures with missing values:\")\n",
    "        print(feature_missing[feature_missing > 0].sort_values(ascending=False))\n",
    "    else:\n",
    "        print(\"\\nNo missing values in extracted features\")\n",
    "    \n",
    "    # Check for constant features\n",
    "    constant_features = []\n",
    "    for col in features_df.columns:\n",
    "        if features_df[col].nunique() <= 1:\n",
    "            constant_features.append(col)\n",
    "    \n",
    "    if constant_features:\n",
    "        print(f\"\\nConstant features (consider removing): {constant_features}\")\n",
    "    else:\n",
    "        print(\"\\nNo constant features found\")\n",
    "    \n",
    "    # Feature value ranges\n",
    "    print(\"\\nFeature Value Ranges:\")\n",
    "    numeric_features = features_df.select_dtypes(include=[np.number])\n",
    "    print(f\"Numeric features: {len(numeric_features.columns)}\")\n",
    "    print(f\"Min values range: [{numeric_features.min().min():.6f}, {numeric_features.min().max():.6f}]\")\n",
    "    print(f\"Max values range: [{numeric_features.max().min():.6f}, {numeric_features.max().max():.6f}]\")\n",
    "    \n",
    "    # Feature correlations (sample)\n",
    "    print(\"\\nHighly correlated feature pairs (|r| > 0.8):\")\n",
    "    corr_matrix = numeric_features.corr().abs()\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if corr_matrix.iloc[i, j] > 0.8:\n",
    "                high_corr_pairs.append((\n",
    "                    corr_matrix.columns[i], \n",
    "                    corr_matrix.columns[j], \n",
    "                    corr_matrix.iloc[i, j]\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: x[2], reverse=True)[:10]:\n",
    "            print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"  No highly correlated pairs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'features_df' in locals():\n",
    "    print(\"Creating feature visualizations...\")\n",
    "    \n",
    "    # Select a subset of features for visualization\n",
    "    viz_features = [\n",
    "        'hour', 'day_of_week', 'amount_log', 'user_transaction_count',\n",
    "        'user_avg_amount', 'time_since_last_transaction', 'location_frequency',\n",
    "        'device_frequency', 'transaction_type_frequency'\n",
    "    ]\n",
    "    \n",
    "    available_viz_features = [f for f in viz_features if f in features_df.columns]\n",
    "    \n",
    "    if available_viz_features:\n",
    "        # Feature distributions\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i, feature in enumerate(available_viz_features[:9]):\n",
    "            if i < len(axes):\n",
    "                features_df[feature].hist(bins=30, ax=axes[i], alpha=0.7)\n",
    "                axes[i].set_title(f'{feature}')\n",
    "                axes[i].set_xlabel('Value')\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(available_viz_features), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Feature Distributions', y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature correlation heatmap (subset)\n",
    "        if len(available_viz_features) > 1:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            corr_subset = features_df[available_viz_features].corr()\n",
    "            sns.heatmap(corr_subset, annot=True, cmap='coolwarm', center=0,\n",
    "                       square=True, fmt='.2f')\n",
    "            plt.title('Feature Correlation Matrix (Subset)')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No visualization features available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'features_df' in locals():\n",
    "    print(\"Validating extracted features...\")\n",
    "    \n",
    "    # Run feature validation\n",
    "    validation_results = feature_extractor.validate_features(features_df, metadata)\n",
    "    \n",
    "    print(\"\\nValidation Results:\")\n",
    "    for category, results in validation_results.items():\n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        for check, result in results.items():\n",
    "            status = \"✓\" if result['passed'] else \"✗\"\n",
    "            print(f\"  {status} {check}: {result['message']}\")\n",
    "            if not result['passed'] and 'details' in result:\n",
    "                print(f\"    Details: {result['details']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'features_df' in locals():\n",
    "    # Save features to CSV\n",
    "    output_path = '../data/engineered_features.csv'\n",
    "    features_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved {len(features_df)} rows and {len(features_df.columns)} features to {output_path}\")\n",
    "    \n",
    "    # Save feature metadata\n",
    "    metadata_path = '../data/feature_metadata.json'\n",
    "    import json\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"Saved feature metadata to {metadata_path}\")\n",
    "    \n",
    "    # Create feature summary\n",
    "    feature_summary = {\n",
    "        'total_features': len(features_df.columns),\n",
    "        'total_samples': len(features_df),\n",
    "        'feature_categories': {cat: len(feats) for cat, feats in metadata.items()},\n",
    "        'numeric_features': len(features_df.select_dtypes(include=[np.number]).columns),\n",
    "        'categorical_features': len(features_df.select_dtypes(exclude=[np.number]).columns),\n",
    "        'missing_values': int(features_df.isnull().sum().sum()),\n",
    "        'constant_features': len(constant_features) if 'constant_features' in locals() else 0,\n",
    "        'creation_time': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    summary_path = '../data/feature_engineering_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(feature_summary, f, indent=2)\n",
    "    print(f\"Saved feature engineering summary to {summary_path}\")\n",
    "    \n",
    "    print(\"\\nFeature Engineering Summary:\")\n",
    "    for key, value in feature_summary.items():\n",
    "        if key != 'creation_time':\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Engineering Complete!\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Run model training notebook to train anomaly detection models\")\n",
    "print(\"2. Evaluate model performance using the extracted features\")\n",
    "print(\"3. Fine-tune feature selection based on model performance\")\n",
    "print(\"4. Deploy the best performing model for fraud detection\")\n",
    "\n",
    "if 'features_df' in locals():\n",
    "    print(f\"\\nReady for model training with {len(features_df.columns)} features and {len(features_df)} samples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
