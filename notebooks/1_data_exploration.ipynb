{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Parsing Analysis\n",
    "\n",
    "This notebook explores the synthetic transaction logs dataset and tests our robust parser implementation.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and explore the raw dataset\n",
    "2. Test the log parser on various formats\n",
    "3. Analyze parsing success rates and data quality\n",
    "4. Create visualizations for data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "from parser.log_parser import TransactionLogParser\n",
    "from utils.config import config\n",
    "from utils.visualization import viz\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"Configuration loaded: {config.get('data.input_file')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "data_file = '../synthetic_dirty_transaction_logs.csv'\n",
    "df_raw = pd.read_csv(data_file, delimiter='|', header=None, names=['line_num', 'raw_log'])\n",
    "\n",
    "print(f\"📊 Dataset Overview:\")\n",
    "print(f\"Total entries: {len(df_raw):,}\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")\n",
    "print(f\"Memory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n📈 Raw Data Info:\")\n",
    "print(df_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample different types of log entries\n",
    "print(\"🔍 Sample Log Entries:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get non-null, non-empty entries\n",
    "valid_logs = df_raw[df_raw['raw_log'].notna() & \n",
    "                   (df_raw['raw_log'] != '\"\"') & \n",
    "                   (df_raw['raw_log'] != 'MALFORMED_LOG')]['raw_log']\n",
    "\n",
    "# Show first 10 valid log entries to understand formats\n",
    "for i, log_entry in enumerate(valid_logs.head(10), 1):\n",
    "    print(f\"{i:2d}. {log_entry}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze log entry patterns\n",
    "print(\"📋 Log Entry Pattern Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Count different patterns\n",
    "pattern_counts = {\n",
    "    'Empty (\"\")': len(df_raw[df_raw['raw_log'] == '\"\"']),\n",
    "    'Malformed': len(df_raw[df_raw['raw_log'] == 'MALFORMED_LOG']),\n",
    "    'Null/NaN': len(df_raw[df_raw['raw_log'].isna()]),\n",
    "    'With |': len(df_raw[df_raw['raw_log'].str.contains('\\|', na=False)]),\n",
    "    'With ::': len(df_raw[df_raw['raw_log'].str.contains('::', na=False)]),\n",
    "    'With >>>': len(df_raw[df_raw['raw_log'].str.contains('>>', na=False)]),\n",
    "    'With :::': len(df_raw[df_raw['raw_log'].str.contains(':::', na=False)]),\n",
    "    'With usr:': len(df_raw[df_raw['raw_log'].str.contains('usr:', na=False)]),\n",
    "    'Space separated': len(df_raw[df_raw['raw_log'].str.startswith('user', na=False)])\n",
    "}\n",
    "\n",
    "for pattern, count in pattern_counts.items():\n",
    "    percentage = (count / len(df_raw)) * 100\n",
    "    print(f\"{pattern:15s}: {count:5d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal valid logs: {len(valid_logs):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Log Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parser\n",
    "parser = TransactionLogParser()\n",
    "print(\"🔧 Parser initialized successfully!\")\n",
    "\n",
    "# Test parser on sample entries\n",
    "print(\"\\n🧪 Testing Parser on Sample Entries:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_entries = [\n",
    "    \"2025-07-05 19:18:10::user1069::withdrawal::2995.12::London::iPhone 13\",\n",
    "    \"usr:user1076|cashout|€4821.85|Glasgow|2025-07-15 12:56:05|Pixel 6\",\n",
    "    \"2025-07-20 05:38:14 >> [user1034] did top-up - amt=€2191.06 - None // dev:iPhone 13\",\n",
    "    \"13/07/2025 14:53:36 ::: user1048 *** TOP-UP ::: amt:3248.15£ @ Manchester <iPhone 13>\",\n",
    "    \"2025-06-23 14:45:58 - user=user1075 - action=debit $1215.74 - ATM: Leeds - device=Samsung Galaxy S10\"\n",
    "]\n",
    "\n",
    "for i, entry in enumerate(test_entries, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Raw: {entry}\")\n",
    "    \n",
    "    result = parser.parse_log_entry(entry)\n",
    "    print(f\"Parsed: {result.is_parsed}\")\n",
    "    \n",
    "    if result.is_parsed:\n",
    "        print(f\"  User: {result.user_id}\")\n",
    "        print(f\"  Type: {result.transaction_type}\")\n",
    "        print(f\"  Amount: {result.currency}{result.amount}\")\n",
    "        print(f\"  Location: {result.location}\")\n",
    "        print(f\"  Device: {result.device}\")\n",
    "        print(f\"  Timestamp: {result.timestamp}\")\n",
    "    else:\n",
    "        print(f\"  Errors: {result.parse_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the complete dataset\n",
    "print(\"🚀 Parsing complete dataset...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "df_parsed, parsing_stats = parser.parse_dataset(data_file)\n",
    "\n",
    "print(\"\\n✅ Parsing Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total logs processed: {parsing_stats['total_logs']:,}\")\n",
    "print(f\"Successfully parsed: {parsing_stats['parsed_successfully']:,}\")\n",
    "print(f\"Parsing failed: {parsing_stats['parsing_failed']:,}\")\n",
    "print(f\"Empty logs: {parsing_stats['empty_logs']:,}\")\n",
    "print(f\"Malformed logs: {parsing_stats['malformed_logs']:,}\")\n",
    "\n",
    "success_rate = (parsing_stats['parsed_successfully'] / parsing_stats['total_logs']) * 100\n",
    "print(f\"\\n📈 Overall Success Rate: {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive parsing statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 COMPREHENSIVE PARSING STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "parser.print_parsing_statistics(parsing_stats)\n",
    "\n",
    "# Export detailed statistics report\n",
    "parser.export_statistics_report(parsing_stats, '../results/detailed_parsing_report.json')\n",
    "print(\"\\n💾 Detailed parsing report saved to results/detailed_parsing_report.json\")\n",
    "\n",
    "# Create and display summary table\n",
    "summary_table = parser.get_parsing_summary_table(parsing_stats)\n",
    "print(\"\\n📋 Parsing Summary Table:\")\n",
    "print(summary_table.to_string(index=False))\n",
    "\n",
    "# Display parsed data overview\n",
    "print(\"\\n📊 Parsed Dataset Overview:\")\n",
    "print(df_parsed.info())\n",
    "\n",
    "print(\"\\n🎯 Successfully Parsed Data:\")\n",
    "df_valid = df_parsed[df_parsed['is_parsed'] == True]\n",
    "print(f\"Valid transactions: {len(df_valid):,}\")\n",
    "print(f\"Date range: {df_valid['timestamp'].min()} to {df_valid['timestamp'].max()}\")\n",
    "print(f\"Unique users: {df_valid['user_id'].nunique():,}\")\n",
    "print(f\"Transaction types: {df_valid['transaction_type'].nunique()}\")\n",
    "print(f\"Unique locations: {df_valid['location'].nunique()}\")\n",
    "print(f\"Unique devices: {df_valid['device'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data quality visualization\n",
    "fig = viz.plot_data_quality_report(df_valid, parsing_stats)\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('../results/data_quality_report.png', dpi=300, bbox_inches='tight')\n",
    "print(\"💾 Data quality report saved to results/data_quality_report.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed data completeness analysis\n",
    "print(\"🔍 Data Completeness Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "completeness = {}\n",
    "for col in ['timestamp', 'user_id', 'transaction_type', 'amount', 'currency', 'location', 'device']:\n",
    "    non_null = df_valid[col].notna().sum()\n",
    "    total = len(df_valid)\n",
    "    completeness[col] = (non_null / total) * 100\n",
    "    print(f\"{col:16s}: {non_null:5d}/{total:5d} ({completeness[col]:.1f}%)\")\n",
    "\n",
    "print(f\"\\nOverall completeness score: {np.mean(list(completeness.values())):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction type distribution\n",
    "print(\"💳 Transaction Type Distribution:\")\n",
    "type_counts = df_valid['transaction_type'].value_counts()\n",
    "print(type_counts)\n",
    "\n",
    "# Currency distribution\n",
    "print(\"\\n💰 Currency Distribution:\")\n",
    "currency_counts = df_valid['currency'].value_counts()\n",
    "print(currency_counts)\n",
    "\n",
    "# Location distribution\n",
    "print(\"\\n🌍 Location Distribution:\")\n",
    "location_counts = df_valid['location'].value_counts()\n",
    "print(location_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add temporal features\n",
    "df_valid = df_valid.copy()\n",
    "df_valid['hour'] = df_valid['timestamp'].dt.hour\n",
    "df_valid['day_of_week'] = df_valid['timestamp'].dt.dayofweek\n",
    "df_valid['is_weekend'] = df_valid['day_of_week'].isin([5, 6])\n",
    "\n",
    "# Plot temporal patterns\n",
    "fig = viz.plot_temporal_patterns(df_valid)\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('../results/temporal_patterns.png', dpi=300, bbox_inches='tight')\n",
    "print(\"💾 Temporal patterns saved to results/temporal_patterns.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. User Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User behavior analysis\n",
    "fig = viz.plot_user_behavior_analysis(df_valid)\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('../results/user_behavior_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"💾 User behavior analysis saved to results/user_behavior_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User statistics\n",
    "print(\"👤 User Statistics:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "user_stats = df_valid.groupby('user_id').agg({\n",
    "    'amount': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'location': 'nunique',\n",
    "    'device': 'nunique',\n",
    "    'transaction_type': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "user_stats.columns = ['tx_count', 'avg_amount', 'std_amount', 'min_amount', 'max_amount', \n",
    "                     'unique_locations', 'unique_devices', 'unique_types']\n",
    "\n",
    "print(\"Summary statistics for all users:\")\n",
    "print(user_stats.describe())\n",
    "\n",
    "print(\"\\nTop 10 most active users:\")\n",
    "print(user_stats.sort_values('tx_count', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Amount Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount distribution analysis\n",
    "print(\"💵 Amount Analysis:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "amount_stats = df_valid['amount'].describe()\n",
    "print(\"Amount statistics:\")\n",
    "print(amount_stats)\n",
    "\n",
    "# Plot amount distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Transaction Amount Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(df_valid['amount'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Amount Distribution')\n",
    "axes[0, 0].set_xlabel('Amount')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Log scale histogram\n",
    "axes[0, 1].hist(np.log1p(df_valid['amount']), bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "axes[0, 1].set_title('Amount Distribution (Log Scale)')\n",
    "axes[0, 1].set_xlabel('Log(Amount + 1)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot by transaction type\n",
    "df_valid.boxplot(column='amount', by='transaction_type', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Amount by Transaction Type')\n",
    "axes[1, 0].set_xlabel('Transaction Type')\n",
    "axes[1, 0].set_ylabel('Amount')\n",
    "\n",
    "# Box plot by currency\n",
    "df_valid.boxplot(column='amount', by='currency', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Amount by Currency')\n",
    "axes[1, 1].set_xlabel('Currency')\n",
    "axes[1, 1].set_ylabel('Amount')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('../results/amount_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"💾 Amount analysis saved to results/amount_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# Save processed data\n",
    "df_valid.to_csv('../results/parsed_transactions.csv', index=False)\n",
    "print(f\"💾 Parsed transactions saved: {len(df_valid):,} records\")\n",
    "\n",
    "# Save parsing statistics\n",
    "import json\n",
    "with open('../results/parsing_stats.json', 'w') as f:\n",
    "    # Convert datetime objects to strings for JSON serialization\n",
    "    json_stats = parsing_stats.copy()\n",
    "    json.dump(json_stats, f, indent=2)\n",
    "print(\"💾 Parsing statistics saved\")\n",
    "\n",
    "print(\"\\n✅ Data exploration complete!\")\n",
    "print(f\"Ready for feature engineering with {len(df_valid):,} clean transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "1. **Parsing Success**: Our robust parser achieved a high success rate in handling multiple log formats\n",
    "2. **Data Quality**: The parsed dataset shows good completeness across key fields\n",
    "3. **Temporal Patterns**: Clear patterns in transaction timing and user behavior\n",
    "4. **User Diversity**: Wide variety in user transaction patterns and amounts\n",
    "5. **Business Context**: Data represents realistic financial transaction patterns\n",
    "\n",
    "### Next Steps:\n",
    "1. Feature engineering based on discovered patterns\n",
    "2. Behavioral modeling for anomaly detection\n",
    "3. Implementation of multiple detection approaches\n",
    "4. Model evaluation and optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
