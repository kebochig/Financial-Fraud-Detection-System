"""
Statistical and clustering-based anomaly detection models.
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.neighbors import LocalOutlierFactor
import hdbscan
from typing import Dict, List, Tuple, Any, Optional
import logging
from ..utils.config import config

logger = logging.getLogger(__name__)

class StatisticalAnomalyDetector:
    """Statistical and clustering-based anomaly detection for fraud detection."""
    
    def __init__(self):
        """Initialize statistical detector with configuration."""
        self.models = {}
        self.scalers = {}
        self.encoders = {}
        self.is_fitted = False
        
        # Load configuration
        self.config = {
            'isolation_forest': config.get('models.isolation_forest', {}),
            'dbscan': config.get('models.dbscan', {}),
            'random_seed': config.get('data.random_seed', 42)
        }
        
        # Initialize models
        self._initialize_models()
        
        logger.info("Statistical anomaly detector initialized")
    
    def _initialize_models(self):
        """Initialize statistical models with configuration."""
        
        # Isolation Forest
        self.models['isolation_forest'] = IsolationForest(
            contamination=self.config['isolation_forest'].get('contamination', 0.1),
            n_estimators=self.config['isolation_forest'].get('n_estimators', 100),
            random_state=self.config['random_seed'],
            n_jobs=-1
        )
        
        # DBSCAN
        self.models['dbscan'] = DBSCAN(
            eps=self.config['dbscan'].get('eps', 0.5),
            min_samples=self.config['dbscan'].get('min_samples', 5),
            n_jobs=-1
        )
        
        # HDBSCAN (more robust than DBSCAN)
        self.models['hdbscan'] = hdbscan.HDBSCAN(
            min_cluster_size=10,
            min_samples=5,
            cluster_selection_epsilon=0.5
        )
        
        # One-Class SVM
        self.models['one_class_svm'] = OneClassSVM(
            kernel='rbf',
            gamma='scale',
            nu=0.1  # Expected fraction of outliers
        )
        
        # Local Outlier Factor
        self.models['lof'] = LocalOutlierFactor(
            n_neighbors=20,
            contamination=0.1,
            novelty=True,
            n_jobs=-1
        )
        
        logger.info(f"Initialized {len(self.models)} statistical models")
    
    def _prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:
        """Prepare features for statistical models."""
        logger.info("Preparing features for statistical models...")
        
        # Select numerical features
        numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # Remove irrelevant columns
        exclude_cols = [
            'line_num', 'is_parsed', 'user_min', 'user_max', 
            'feature_extraction_timestamp'
        ]
        numerical_cols = [col for col in numerical_cols if col not in exclude_cols]
        
        # Handle categorical features
        categorical_cols = []
        for col in ['location_filled', 'device_filled', 'transaction_type_filled', 
                   'currency_filled', 'device_brand', 'transaction_group', 'hour_category']:
            if col in df.columns:
                categorical_cols.append(col)
        
        # Prepare numerical features
        X_numerical = df[numerical_cols].fillna(0).values
        
        # Prepare categorical features
        X_categorical = np.array([])\n        if categorical_cols:\n            X_categorical = np.column_stack([\n                self._encode_categorical(df[col]) for col in categorical_cols\n            ])\n        \n        # Combine features\n        if X_categorical.size > 0:\n            X = np.column_stack([X_numerical, X_categorical])\n            feature_names = numerical_cols + [f\"{col}_encoded\" for col in categorical_cols]\n        else:\n            X = X_numerical\n            feature_names = numerical_cols\n        \n        # Handle infinite values\n        X = np.where(np.isinf(X), 0, X)\n        \n        logger.info(f\"Prepared {X.shape[1]} features for {X.shape[0]} samples\")\n        return X, feature_names\n    \n    def _encode_categorical(self, series: pd.Series) -> np.ndarray:\n        \"\"\"Encode categorical series.\"\"\"\n        series_name = series.name\n        \n        if series_name not in self.encoders:\n            self.encoders[series_name] = LabelEncoder()\n            # Fit and transform\n            encoded = self.encoders[series_name].fit_transform(series.fillna('unknown').astype(str))\n        else:\n            # Transform only\n            try:\n                encoded = self.encoders[series_name].transform(series.fillna('unknown').astype(str))\n            except ValueError:\n                # Handle unseen categories\n                known_categories = set(self.encoders[series_name].classes_)\n                series_cleaned = series.fillna('unknown').astype(str)\n                series_cleaned = series_cleaned.apply(\n                    lambda x: x if x in known_categories else 'unknown'\n                )\n                encoded = self.encoders[series_name].transform(series_cleaned)\n        \n        return encoded.astype(float)\n    \n    def fit(self, df: pd.DataFrame) -> 'StatisticalAnomalyDetector':\n        \"\"\"Fit statistical models on training data.\"\"\"\n        logger.info(f\"Fitting statistical models on {len(df)} samples...\")\n        \n        # Prepare features\n        X, feature_names = self._prepare_features(df)\n        \n        # Scale features\n        self.scalers['standard'] = StandardScaler()\n        X_scaled = self.scalers['standard'].fit_transform(X)\n        \n        # Fit models\n        fitted_models = []\n        \n        # Isolation Forest\n        try:\n            self.models['isolation_forest'].fit(X_scaled)\n            fitted_models.append('isolation_forest')\n            logger.info(\"✅ Isolation Forest fitted\")\n        except Exception as e:\n            logger.error(f\"❌ Error fitting Isolation Forest: {str(e)}\")\n        \n        # One-Class SVM (can be slow on large datasets)\n        if len(df) < 10000:  # Only fit if dataset is not too large\n            try:\n                self.models['one_class_svm'].fit(X_scaled)\n                fitted_models.append('one_class_svm')\n                logger.info(\"✅ One-Class SVM fitted\")\n            except Exception as e:\n                logger.error(f\"❌ Error fitting One-Class SVM: {str(e)}\")\n        else:\n            logger.info(\"⚠️ Skipping One-Class SVM due to large dataset size\")\n        \n        # Local Outlier Factor\n        try:\n            self.models['lof'].fit(X_scaled)\n            fitted_models.append('lof')\n            logger.info(\"✅ Local Outlier Factor fitted\")\n        except Exception as e:\n            logger.error(f\"❌ Error fitting Local Outlier Factor: {str(e)}\")\n        \n        # Store feature names and fitted status\n        self.feature_names = feature_names\n        self.fitted_models = fitted_models\n        self.is_fitted = True\n        \n        logger.info(f\"Statistical models fitting complete. {len(fitted_models)} models fitted.\")\n        return self\n    \n    def predict_anomalies(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n        \"\"\"Predict anomalies using fitted statistical models.\"\"\"\n        if not self.is_fitted:\n            raise ValueError(\"Models must be fitted before prediction\")\n        \n        logger.info(f\"Predicting anomalies for {len(df)} samples...\")\n        \n        # Prepare features\n        X, _ = self._prepare_features(df)\n        X_scaled = self.scalers['standard'].transform(X)\n        \n        predictions = {}\n        \n        # Isolation Forest\n        if 'isolation_forest' in self.fitted_models:\n            try:\n                # Get anomaly scores (lower values = more anomalous)\n                scores = self.models['isolation_forest'].decision_function(X_scaled)\n                # Convert to 0-1 range (higher values = more anomalous)\n                predictions['isolation_forest'] = self._normalize_scores(-scores)\n                logger.debug(\"✅ Isolation Forest predictions generated\")\n            except Exception as e:\n                logger.error(f\"❌ Error in Isolation Forest prediction: {str(e)}\")\n                predictions['isolation_forest'] = np.zeros(len(df))\n        \n        # One-Class SVM\n        if 'one_class_svm' in self.fitted_models:\n            try:\n                scores = self.models['one_class_svm'].decision_function(X_scaled)\n                predictions['one_class_svm'] = self._normalize_scores(-scores)\n                logger.debug(\"✅ One-Class SVM predictions generated\")\n            except Exception as e:\n                logger.error(f\"❌ Error in One-Class SVM prediction: {str(e)}\")\n                predictions['one_class_svm'] = np.zeros(len(df))\n        \n        # Local Outlier Factor\n        if 'lof' in self.fitted_models:\n            try:\n                scores = self.models['lof'].decision_function(X_scaled)\n                predictions['lof'] = self._normalize_scores(-scores)\n                logger.debug(\"✅ Local Outlier Factor predictions generated\")\n            except Exception as e:\n                logger.error(f\"❌ Error in Local Outlier Factor prediction: {str(e)}\")\n                predictions['lof'] = np.zeros(len(df))\n        \n        # Clustering-based detection\n        predictions.update(self._cluster_based_detection(X_scaled))\n        \n        logger.info(f\"Statistical anomaly detection complete. Generated {len(predictions)} score sets.\")\n        return predictions\n    \n    def _cluster_based_detection(self, X_scaled: np.ndarray) -> Dict[str, np.ndarray]:\n        \"\"\"Perform clustering-based anomaly detection.\"\"\"\n        cluster_predictions = {}\n        \n        # DBSCAN clustering\n        try:\n            dbscan_labels = self.models['dbscan'].fit_predict(X_scaled)\n            # Points labeled as -1 are considered anomalies\n            dbscan_scores = (dbscan_labels == -1).astype(float)\n            \n            # For non-anomalous points, calculate distance to cluster center\n            if len(np.unique(dbscan_labels)) > 1:\n                cluster_scores = np.zeros(len(X_scaled))\n                for cluster_id in np.unique(dbscan_labels):\n                    if cluster_id != -1:  # Skip noise points\n                        cluster_mask = dbscan_labels == cluster_id\n                        cluster_center = X_scaled[cluster_mask].mean(axis=0)\n                        distances = np.linalg.norm(\n                            X_scaled[cluster_mask] - cluster_center, axis=1\n                        )\n                        cluster_scores[cluster_mask] = distances\n                \n                # Normalize and combine with noise detection\n                if cluster_scores.max() > 0:\n                    cluster_scores = cluster_scores / cluster_scores.max()\n                \n                dbscan_scores = np.maximum(dbscan_scores, cluster_scores * 0.5)\n            \n            cluster_predictions['dbscan'] = dbscan_scores\n            logger.debug(f\"✅ DBSCAN: {np.sum(dbscan_labels == -1)} anomalies detected\")\n            \n        except Exception as e:\n            logger.error(f\"❌ Error in DBSCAN clustering: {str(e)}\")\n            cluster_predictions['dbscan'] = np.zeros(len(X_scaled))\n        \n        # HDBSCAN clustering\n        try:\n            hdbscan_labels = self.models['hdbscan'].fit_predict(X_scaled)\n            \n            # Use outlier scores if available\n            if hasattr(self.models['hdbscan'], 'outlier_scores_'):\n                hdbscan_scores = self.models['hdbscan'].outlier_scores_\n                hdbscan_scores = np.nan_to_num(hdbscan_scores, 0)\n            else:\n                # Fallback to binary classification\n                hdbscan_scores = (hdbscan_labels == -1).astype(float)\n            \n            cluster_predictions['hdbscan'] = self._normalize_scores(hdbscan_scores)\n            logger.debug(f\"✅ HDBSCAN: {np.sum(hdbscan_labels == -1)} anomalies detected\")\n            \n        except Exception as e:\n            logger.error(f\"❌ Error in HDBSCAN clustering: {str(e)}\")\n            cluster_predictions['hdbscan'] = np.zeros(len(X_scaled))\n        \n        return cluster_predictions\n    \n    def _normalize_scores(self, scores: np.ndarray) -> np.ndarray:\n        \"\"\"Normalize scores to [0, 1] range.\"\"\"\n        scores = np.array(scores)\n        \n        # Handle edge cases\n        if len(scores) == 0:\n            return scores\n        \n        if np.all(scores == scores[0]):  # All values are the same\n            return np.zeros_like(scores)\n        \n        # Min-max normalization\n        min_score = np.min(scores)\n        max_score = np.max(scores)\n        \n        if max_score == min_score:\n            return np.zeros_like(scores)\n        \n        normalized = (scores - min_score) / (max_score - min_score)\n        return np.clip(normalized, 0, 1)\n    \n    def get_feature_importance(self, df: pd.DataFrame, method: str = 'isolation_forest') -> Dict[str, float]:\n        \"\"\"Get feature importance for anomaly detection.\"\"\"\n        if not self.is_fitted:\n            raise ValueError(\"Models must be fitted before getting feature importance\")\n        \n        if method not in self.fitted_models:\n            raise ValueError(f\"Method '{method}' not available. Available: {self.fitted_models}\")\n        \n        # For tree-based methods like Isolation Forest\n        if method == 'isolation_forest' and hasattr(self.models[method], 'feature_importances_'):\n            importances = self.models[method].feature_importances_\n            return dict(zip(self.feature_names, importances))\n        \n        # For other methods, use permutation importance approximation\n        logger.info(f\"Calculating feature importance for {method}...\")\n        \n        X, _ = self._prepare_features(df)\n        X_scaled = self.scalers['standard'].transform(X)\n        \n        # Get baseline scores\n        baseline_scores = self._get_model_scores(method, X_scaled)\n        \n        importances = {}\n        for i, feature_name in enumerate(self.feature_names):\n            # Permute feature\n            X_permuted = X_scaled.copy()\n            np.random.shuffle(X_permuted[:, i])\n            \n            # Get scores with permuted feature\n            permuted_scores = self._get_model_scores(method, X_permuted)\n            \n            # Calculate importance as change in mean score\n            importance = np.mean(np.abs(permuted_scores - baseline_scores))\n            importances[feature_name] = importance\n        \n        # Normalize importances\n        total_importance = sum(importances.values())\n        if total_importance > 0:\n            importances = {k: v / total_importance for k, v in importances.items()}\n        \n        return importances\n    \n    def _get_model_scores(self, method: str, X: np.ndarray) -> np.ndarray:\n        \"\"\"Get anomaly scores from a specific model.\"\"\"\n        if method == 'isolation_forest':\n            return -self.models[method].decision_function(X)\n        elif method == 'one_class_svm':\n            return -self.models[method].decision_function(X)\n        elif method == 'lof':\n            return -self.models[method].decision_function(X)\n        else:\n            # For clustering methods, return binary scores\n            return np.zeros(len(X))\n    \n    def detect_anomalies_ensemble(self, df: pd.DataFrame, weights: Dict[str, float] = None) -> np.ndarray:\n        \"\"\"Detect anomalies using ensemble of statistical methods.\"\"\"\n        predictions = self.predict_anomalies(df)\n        \n        if weights is None:\n            # Default equal weights\n            weights = {method: 1.0 / len(predictions) for method in predictions.keys()}\n        \n        # Normalize weights\n        total_weight = sum(weights.values())\n        weights = {k: v / total_weight for k, v in weights.items()}\n        \n        # Compute weighted ensemble score\n        ensemble_scores = np.zeros(len(df))\n        for method, scores in predictions.items():\n            method_weight = weights.get(method, 0)\n            ensemble_scores += scores * method_weight\n        \n        logger.info(f\"Ensemble anomaly detection complete. Mean score: {np.mean(ensemble_scores):.4f}\")\n        return ensemble_scores\n    \n    def get_model_statistics(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get statistics for each statistical model.\"\"\"\n        if not self.is_fitted:\n            raise ValueError(\"Models must be fitted before getting statistics\")\n        \n        predictions = self.predict_anomalies(df)\n        statistics = {}\n        \n        for method, scores in predictions.items():\n            statistics[method] = {\n                'mean_score': float(np.mean(scores)),\n                'std_score': float(np.std(scores)),\n                'min_score': float(np.min(scores)),\n                'max_score': float(np.max(scores)),\n                'anomaly_count_10pct': int(np.sum(scores > np.percentile(scores, 90))),\n                'anomaly_count_5pct': int(np.sum(scores > np.percentile(scores, 95))),\n                'anomaly_count_1pct': int(np.sum(scores > np.percentile(scores, 99)))\n            }\n        \n        return statistics\n    \n    def reduce_dimensionality(self, df: pd.DataFrame, n_components: int = 2) -> Tuple[np.ndarray, PCA]:\n        \"\"\"Reduce dimensionality for visualization.\"\"\"\n        X, _ = self._prepare_features(df)\n        X_scaled = self.scalers['standard'].transform(X) if self.is_fitted else StandardScaler().fit_transform(X)\n        \n        pca = PCA(n_components=n_components, random_state=self.config['random_seed'])\n        X_reduced = pca.fit_transform(X_scaled)\n        \n        logger.info(f\"Reduced dimensionality to {n_components} components. \"\n                   f\"Explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n        \n        return X_reduced, pca
