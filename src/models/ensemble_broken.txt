"""
Ensemble anomaly detection combining multiple approaches.
"""
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import logging
from ..utils.config import config
from .rule_based import RuleBasedAnomalyDetector
from .statistical import StatisticalAnomalyDetector

logger = logging.getLogger(__name__)

class EnsembleAnomalyDetector:
    """Ensemble fraud detection system combining multiple approaches."""
    
    def __init__(self):
        """Initialize ensemble detector with configuration."""
        
        # Load ensemble configuration
        self.ensemble_config = config.get('models.ensemble', {
            'rule_weight': 0.3,
            'isolation_weight': 0.25,
            'autoencoder_weight': 0.25,
            'clustering_weight': 0.2
        })
        
        # Initialize component detectors
        self.rule_detector = RuleBasedAnomalyDetector()
        self.statistical_detector = StatisticalAnomalyDetector()
        
        # Ensemble state
        self.is_fitted = False
        self.component_scores = {}
        self.feature_importance = {}
        
        logger.info("Ensemble anomaly detector initialized")
    
    def fit(self, df: pd.DataFrame) -> 'EnsembleAnomalyDetector':
        """Fit all component detectors."""
        logger.info(f"Fitting ensemble detector on {len(df)} samples...")
        
        # Fit statistical models
        self.statistical_detector.fit(df)
        
        # Rule-based detector doesn't need fitting
        logger.info("✅ Rule-based detector ready")
        
        self.is_fitted = True
        logger.info("✅ Ensemble detector fitting complete")
        return self
    
    def detect_anomalies(self, df: pd.DataFrame) -> Tuple[np.ndarray, Dict[str, Any]]:
        """Detect anomalies using ensemble approach."""
        if not self.is_fitted:
            raise ValueError("Ensemble must be fitted before detection")
        
        logger.info(f"Running ensemble anomaly detection on {len(df)} samples...")
        
        # Get predictions from all components
        component_results = {}
        
        # Rule-based detection
        logger.info("🔍 Running rule-based detection...")
        rule_scores, rule_details = self.rule_detector.detect_anomalies(df)
        component_results['rule_based'] = {
            'scores': rule_scores,
            'details': rule_details
        }
        
        # Statistical detection
        logger.info("📊 Running statistical detection...")
        statistical_predictions = self.statistical_detector.predict_anomalies(df)
        component_results['statistical'] = {
            'scores': statistical_predictions,
            'details': statistical_predictions
        }
        
        # Combine scores using weighted ensemble
        ensemble_scores = self._combine_scores(component_results)
        
        # Store component scores for analysis
        self.component_scores = component_results
        
        # Create ensemble results
        ensemble_results = {
            'component_scores': component_results,
            'ensemble_weights': self.ensemble_config,
            'mean_score': float(np.mean(ensemble_scores)),
            'std_score': float(np.std(ensemble_scores)),
            'anomaly_count_threshold_70': int(np.sum(ensemble_scores > 0.7)),
            'anomaly_count_threshold_80': int(np.sum(ensemble_scores > 0.8)),
            'anomaly_count_threshold_90': int(np.sum(ensemble_scores > 0.9))
        }
        
        logger.info(f"Ensemble detection complete. Mean score: {ensemble_results['mean_score']:.4f}")
        return ensemble_scores, ensemble_results
    
    def _combine_scores(self, component_results: Dict[str, Any]) -> np.ndarray:
        """Combine component scores using weighted ensemble."""
        logger.info("🔗 Combining component scores...")
        
        # Get rule-based scores
        rule_scores = component_results['rule_based']['scores']
        
        # Get statistical scores (use isolation forest as primary)
        statistical_scores = component_results['statistical']['scores']
        isolation_scores = statistical_scores.get('isolation_forest', np.zeros(len(rule_scores)))
        
        # Get clustering scores (use DBSCAN as primary)
        clustering_scores = statistical_scores.get('dbscan', np.zeros(len(rule_scores)))
        
        # For autoencoder, we'll use LOF as a proxy for now
        # In a full implementation, this would be a separate autoencoder model
        autoencoder_scores = statistical_scores.get('lof', np.zeros(len(rule_scores)))
        
        # Weighted combination
        ensemble_scores = (\n            rule_scores * self.ensemble_config['rule_weight'] +\n            isolation_scores * self.ensemble_config['isolation_weight'] +\n            autoencoder_scores * self.ensemble_config['autoencoder_weight'] +\n            clustering_scores * self.ensemble_config['clustering_weight']\n        )\n        \n        # Normalize to [0, 1] range\n        ensemble_scores = np.clip(ensemble_scores, 0, 1)\n        \n        logger.info(f\"Ensemble combination complete. Score range: [{np.min(ensemble_scores):.3f}, {np.max(ensemble_scores):.3f}]\")\n        return ensemble_scores\n    \n    def explain_anomalies(self, df: pd.DataFrame, ensemble_scores: np.ndarray, \n                         top_n: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Explain top anomalies with contributions from each component.\"\"\"\n        logger.info(f\"Explaining top {top_n} anomalies...\")\n        \n        # Get top anomaly indices\n        top_indices = np.argsort(ensemble_scores)[-top_n:][::-1]\n        \n        explanations = []\n        \n        for idx in top_indices:\n            explanation = {\n                'index': int(idx),\n                'ensemble_score': float(ensemble_scores[idx]),\n                'transaction_details': df.iloc[idx].to_dict(),\n                'component_contributions': {},\n                'rule_explanations': None,\n                'statistical_details': {}\n            }\n            \n            # Get rule-based explanation\n            if 'rule_based' in self.component_scores:\n                rule_explanation = self.rule_detector.explain_anomaly(\n                    df, idx, self.component_scores['rule_based']['details']\n                )\n                explanation['rule_explanations'] = rule_explanation\n                explanation['component_contributions']['rule_based'] = {\n                    'score': float(self.component_scores['rule_based']['scores'][idx]),\n                    'weight': self.ensemble_config['rule_weight'],\n                    'contribution': float(self.component_scores['rule_based']['scores'][idx] * \n                                        self.ensemble_config['rule_weight'])\n                }\n            \n            # Get statistical contributions\n            if 'statistical' in self.component_scores:\n                statistical_scores = self.component_scores['statistical']['scores']\n                \n                for method, scores in statistical_scores.items():\n                    method_weight = self._get_statistical_weight(method)\n                    explanation['component_contributions'][f'statistical_{method}'] = {\n                        'score': float(scores[idx]),\n                        'weight': method_weight,\n                        'contribution': float(scores[idx] * method_weight)\n                    }\n                    \n                    explanation['statistical_details'][method] = float(scores[idx])\n            \n            explanations.append(explanation)\n        \n        logger.info(f\"Generated explanations for {len(explanations)} anomalies\")\n        return explanations\n    \n    def _get_statistical_weight(self, method: str) -> float:\n        \"\"\"Get weight for statistical method based on ensemble configuration.\"\"\"\n        method_weights = {\n            'isolation_forest': self.ensemble_config['isolation_weight'],\n            'lof': self.ensemble_config['autoencoder_weight'],  # Using LOF as proxy\n            'dbscan': self.ensemble_config['clustering_weight'],\n            'hdbscan': self.ensemble_config['clustering_weight'] * 0.5,\n            'one_class_svm': self.ensemble_config['isolation_weight'] * 0.5\n        }\n        return method_weights.get(method, 0.1)\n    \n    def get_feature_importance(self, df: pd.DataFrame) -> Dict[str, float]:\n        \"\"\"Get combined feature importance from all components.\"\"\"\n        if not self.is_fitted:\n            raise ValueError(\"Ensemble must be fitted before getting feature importance\")\n        \n        logger.info(\"Calculating ensemble feature importance...\")\n        \n        combined_importance = {}\n        \n        # Get statistical feature importance\n        try:\n            stat_importance = self.statistical_detector.get_feature_importance(df)\n            \n            # Weight by ensemble configuration\n            for feature, importance in stat_importance.items():\n                combined_importance[feature] = importance * self.ensemble_config['isolation_weight']\n        \n        except Exception as e:\n            logger.warning(f\"Could not get statistical feature importance: {str(e)}\")\n        \n        # Rule-based feature importance is implicit in rule definitions\n        # We can estimate it based on rule weights and feature usage\n        rule_feature_importance = self._estimate_rule_feature_importance()\n        \n        # Combine with rule-based importance\n        for feature, importance in rule_feature_importance.items():\n            if feature in combined_importance:\n                combined_importance[feature] += importance * self.ensemble_config['rule_weight']\n            else:\n                combined_importance[feature] = importance * self.ensemble_config['rule_weight']\n        \n        # Normalize importance scores\n        total_importance = sum(combined_importance.values())\n        if total_importance > 0:\n            combined_importance = {k: v / total_importance for k, v in combined_importance.items()}\n        \n        # Sort by importance\n        sorted_importance = dict(sorted(combined_importance.items(), \n                                      key=lambda x: x[1], reverse=True))\n        \n        logger.info(f\"Calculated importance for {len(sorted_importance)} features\")\n        return sorted_importance\n    \n    def _estimate_rule_feature_importance(self) -> Dict[str, float]:\n        \"\"\"Estimate feature importance based on rule definitions.\"\"\"\n        # This maps rules to the features they primarily use\n        rule_feature_mapping = {\n            'large_amount': ['amount', 'amount_log', 'amount_category'],\n            'unusual_hours': ['hour', 'is_unusual_hour', 'is_night_transaction'],\n            'high_velocity': ['tx_velocity_24h', 'user_tx_count'],\n            'unusual_location': ['location_rarity', 'location_risk_score'],\n            'device_change': ['device_changed', 'user_device_diversity'],\n            'amount_deviation': ['amount_zscore_user', 'user_amount_cv'],\n            'weekend_activity': ['is_weekend', 'amount'],\n            'rare_transaction_type': ['type_rarity', 'transaction_type'],\n            'multiple_locations': ['user_unique_locations', 'location_diversity'],\n            'round_amounts': ['amount', 'amount_rounded']\n        }\n        \n        feature_importance = {}\n        \n        for rule_name, rule_config in self.rule_detector.rules.items():\n            rule_weight = rule_config['weight']\n            features = rule_feature_mapping.get(rule_name, [])\n            \n            # Distribute rule weight among its features\n            feature_weight = rule_weight / len(features) if features else 0\n            \n            for feature in features:\n                if feature in feature_importance:\n                    feature_importance[feature] += feature_weight\n                else:\n                    feature_importance[feature] = feature_weight\n        \n        return feature_importance\n    \n    def get_ensemble_statistics(self, df: pd.DataFrame, ensemble_scores: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Get comprehensive statistics for the ensemble.\"\"\"\n        statistics = {\n            'ensemble_stats': {\n                'mean_score': float(np.mean(ensemble_scores)),\n                'std_score': float(np.std(ensemble_scores)),\n                'min_score': float(np.min(ensemble_scores)),\n                'max_score': float(np.max(ensemble_scores)),\n                'percentiles': {\n                    '50th': float(np.percentile(ensemble_scores, 50)),\n                    '75th': float(np.percentile(ensemble_scores, 75)),\n                    '90th': float(np.percentile(ensemble_scores, 90)),\n                    '95th': float(np.percentile(ensemble_scores, 95)),\n                    '99th': float(np.percentile(ensemble_scores, 99))\n                }\n            },\n            'component_stats': {},\n            'ensemble_config': self.ensemble_config\n        }\n        \n        # Add component statistics\n        if hasattr(self, 'component_scores'):\n            # Rule-based statistics\n            if 'rule_based' in self.component_scores:\n                rule_scores = self.component_scores['rule_based']['scores']\n                statistics['component_stats']['rule_based'] = {\n                    'mean_score': float(np.mean(rule_scores)),\n                    'triggered_transactions': int(np.sum(rule_scores > 0)),\n                    'trigger_rate': float(np.sum(rule_scores > 0) / len(rule_scores))\n                }\n            \n            # Statistical statistics\n            if 'statistical' in self.component_scores:\n                stat_stats = self.statistical_detector.get_model_statistics(df)\n                statistics['component_stats']['statistical'] = stat_stats\n        \n        return statistics\n    \n    def update_ensemble_weights(self, new_weights: Dict[str, float]):\n        \"\"\"Update ensemble weights.\"\"\"\n        # Validate weights\n        required_keys = ['rule_weight', 'isolation_weight', 'autoencoder_weight', 'clustering_weight']\n        \n        for key in required_keys:\n            if key not in new_weights:\n                raise ValueError(f\"Missing weight: {key}\")\n        \n        # Normalize weights to sum to 1\n        total_weight = sum(new_weights.values())\n        if total_weight <= 0:\n            raise ValueError(\"Total weight must be positive\")\n        \n        normalized_weights = {k: v / total_weight for k, v in new_weights.items()}\n        \n        # Update configuration\n        self.ensemble_config.update(normalized_weights)\n        \n        logger.info(f\"Updated ensemble weights: {self.ensemble_config}\")\n    \n    def get_top_anomalies(self, df: pd.DataFrame, ensemble_scores: np.ndarray, \n                          top_n: int = 100) -> pd.DataFrame:\n        \"\"\"Get top N anomalies with detailed information.\"\"\"\n        # Get top anomaly indices\n        top_indices = np.argsort(ensemble_scores)[-top_n:][::-1]\n        \n        # Create results dataframe\n        top_anomalies = df.iloc[top_indices].copy()\n        top_anomalies['ensemble_score'] = ensemble_scores[top_indices]\n        \n        # Add component scores\n        if hasattr(self, 'component_scores'):\n            if 'rule_based' in self.component_scores:\n                top_anomalies['rule_score'] = self.component_scores['rule_based']['scores'][top_indices]\n            \n            if 'statistical' in self.component_scores:\n                statistical_scores = self.component_scores['statistical']['scores']\n                for method, scores in statistical_scores.items():\n                    top_anomalies[f'{method}_score'] = scores[top_indices]\n        \n        # Add risk categorization\n        top_anomalies['risk_level'] = pd.cut(\n            top_anomalies['ensemble_score'],\n            bins=[0, 0.3, 0.6, 0.8, 1.0],\n            labels=['Low', 'Medium', 'High', 'Critical'],\n            include_lowest=True\n        )\n        \n        return top_anomalies.reset_index(drop=True)\n    \n    def save_model(self, filepath: str):\n        \"\"\"Save ensemble model state.\"\"\"\n        import pickle\n        \n        model_state = {\n            'ensemble_config': self.ensemble_config,\n            'is_fitted': self.is_fitted,\n            'statistical_detector': self.statistical_detector,\n            'rule_detector': self.rule_detector\n        }\n        \n        with open(filepath, 'wb') as f:\n            pickle.dump(model_state, f)\n        \n        logger.info(f\"Ensemble model saved to {filepath}\")\n    \n    def load_model(self, filepath: str):\n        \"\"\"Load ensemble model state.\"\"\"\n        import pickle\n        \n        with open(filepath, 'rb') as f:\n            model_state = pickle.load(f)\n        \n        self.ensemble_config = model_state['ensemble_config']\n        self.is_fitted = model_state['is_fitted']\n        self.statistical_detector = model_state['statistical_detector']\n        self.rule_detector = model_state['rule_detector']\n        \n        logger.info(f\"Ensemble model loaded from {filepath}\")
